# -*- coding: utf-8 -*-
"""M4 - Model Monitoring & Performance Tracking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rc4NR91fAwKnxF9loO0xXXxyCE7hm_Mq

### **M4: Model Monitoring & Performance Tracking**

Objective: Track and monitor model performance over time.

**Tasks:**
- Use an open-source tracking tool (e.g., MLflow, Neptune.ai, Weights & Biases) to log model performance metrics.
- Implement drift detection to identify when retraining is required.

**Deliverables:**
- Performance tracking logs.
- Drift detection implementation.
- Screenshots showing model performance over multiple runs.

Install required libraries for mlflow and drift detection
"""

!pip install mlflow
!pip install scikit-multiflow

! pip install alibi_detect

"""Set the path to store mlruns"""

import os

# Set the path for storing mlruns (you can choose any path)
mlruns_dir = '/content/mlops_mlruns'

# Set the MLFLOW_TRACKING_URI to the directory where mlruns will be stored
os.environ['MLFLOW_TRACKING_URI'] = mlruns_dir

# Make sure the directory exists
os.makedirs(mlruns_dir, exist_ok=True)

!pip install mlflow

!pip install alibi_detect

import numpy as np
import pandas as pd
import mlflow
import mlflow.sklearn
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from alibi_detect.cd import KSDrift
import matplotlib.pyplot as plt
from datetime import datetime
from tensorflow.keras.datasets import fashion_mnist
import os

# --------- Load and Preprocess the Fashion MNIST Dataset ---------
print("\n********** Loading and Preprocessing Fashion MNIST Dataset **********")
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Reshape the data to be 2D (Flatten the images)
x_train_flat = x_train.reshape(x_train.shape[0], -1).astype('float32') / 255.0
x_test_flat = x_test.reshape(x_test.shape[0], -1).astype('float32') / 255.0

# Normalize pixel values to range [0, 1]
print("\nData loaded and reshaped successfully. Preprocessing completed\n")

# --------- Model Definitions ---------
print("\n********** Defining Models for Training **********")
# List of models to be used for training
models = {
    "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42),
    # "LogisticRegression": LogisticRegression(max_iter=1000, random_state=42),
    "SVM": SVC(random_state=42),
    # "KNeighbors": KNeighborsClassifier()
}

# --------- MLflow Setup ---------
print("\n********** Setting Up MLflow for Model Tracking **********")
# Check if the experiment already exists

# Set the experiment
current_dir = os.getcwd()
print(f"Current directory: {current_dir}")

# Set a valid experiment name
experiment_name = 'Fashion_MNIST_Model'

# Check if the experiment exists and create if it doesn't
experiment = mlflow.get_experiment_by_name(experiment_name)

if experiment is None:
    mlflow.create_experiment(experiment_name)

# Now set the experiment
mlflow.set_experiment(experiment_name)

print(f"Experiment '{experiment_name}' is set up successfully.")

print("\nMLflow experiment setup completed.\n")

# --------- Train the Model and Track with MLflow ---------
def train_and_log_model(model, X_train, y_train, X_test, y_test):
    print(f"********** Training and Logging Model: {type(model).__name__} **********")
    with mlflow.start_run(run_name=type(model).__name__):
        # Train the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)

        # Log metrics to MLflow
        mlflow.log_metric('accuracy', accuracy)

        # Create an input_example for model signature (helps remove warning)
        input_example = X_train[0].reshape(1, -1)  # Taking the first sample from X_train

        # Log the model with input_example to auto infer signature
        mlflow.sklearn.log_model(model, 'model', input_example=input_example)

        # Log parameters (could be expanded for more hyperparameters)
        mlflow.log_param('model_name', type(model).__name__)

        # Print the logs
        print(f"\nLogged run for {type(model).__name__} with accuracy: {accuracy}")

        return accuracy

# --------- Drift Detection Setup with Alibi Detect ---------
print("\n********** Drift Detection Setup with Alibi Detect **********")
# We use the KSDrift method from Alibi Detect for drift detection.
def detect_drift(X_train, X_test):
    # Initialize KSDrift detector for dataset shift detection
    drift_detector = KSDrift(X_train, p_val=0.05)  # 0.05 is the significance level

    # Check for drift between the training and test datasets
    drift_result = drift_detector.predict(X_test)

    # Check if drift is detected and extract relevant information
    if 'data' in drift_result and 'is_drift' in drift_result['data']:
        if drift_result['data']['is_drift']:
            print("\n*** Drift Detected ***")
            # Safely accessing the test statistic and p-value from drift_result
            ks_stat = drift_result['data'].get('ks_stat', 'N/A')  # Defaulting to 'N/A' if not present
            p_val = drift_result['data'].get('p_val', 'N/A')  # Defaulting to 'N/A' if not present

            print(f"Drift Type: KS Drift (Kolmogorov-Smirnov Test)")
            print(f"Test Statistic: {ks_stat}")
            print(f"P-value: {p_val}")
            print("Decision: Drift detected, retraining is required.\n")
            return True
        else:
            print("\n*** No Drift Detected ***")
            p_val = drift_result['data'].get('p_val', 'N/A')  # Defaulting to 'N/A' if not present
            print(f"P-value: {p_val}")
            print("Decision: No drift detected.\n")
            return False
    else:
        print("\n*** Drift Detection Result Structure Incomplete ***")
        print("Cannot detect drift. Missing expected keys in result.")
        return False

# --------- Simulate Drift ---------
print("\n********** Simulating Drift in the Test Data **********")
def simulate_drift(X_test):
    """Simulate drift by adding random noise to the test data."""
    # Simulating drift by adding noise to the test data
    noise = np.random.normal(0, 0.1, X_test.shape)  # Adding random noise with mean=0 and std=0.1
    X_test_drifted = X_test + noise  # Simulate the drift by adding the noise to the original test data

    # Clip values to ensure they stay in the range [0, 1]
    X_test_drifted = np.clip(X_test_drifted, 0, 1)  # Ensure that the pixel values remain valid between 0 and 1

    # The above steps simulate a drift in the dataset, causing the distribution of the test data to change.
    return X_test_drifted

# --------- Monitor Performance Over Time ---------
print("\n********** Monitoring Model Performance Over Time **********")
# Track and monitor model performance over time
def monitor_performance():
    accuracies = {model_name: [] for model_name in models}  # To store accuracies for all models
    for model_name, model in models.items():
        print(f"\nTraining and logging the model: {model_name}")

        # Train and log the model with MLflow
        accuracy = train_and_log_model(model, x_train_flat, y_train, x_test_flat, y_test)
        accuracies[model_name].append(accuracy)

        # Simulate drift after each run (simulate changes in the incoming data over time)
        x_test_drifted = simulate_drift(x_test_flat)  # This is where drift is introduced

        # Using Alibi Detect to Detect the drift in the dataset
        drift_detected = detect_drift(x_train_flat, x_test_drifted)

        # If drift is detected, We need to retrain the model
        if drift_detected:
            print(f"Retraining the {model_name} due to detected drift...\n")
            accuracy = train_and_log_model(model, x_train_flat, y_train, x_test_flat, y_test)  # Retrain the model
            accuracies[model_name].append(accuracy)

        # Log performance over time
        performance_log = {
            'run_time': datetime.now(),
            'model': model_name,
            'accuracy': accuracy
        }
        performance_df = pd.DataFrame([performance_log])
        performance_df.to_csv('performance_log.csv', mode='a', header=False, index=False)

    # --------- Plotting Model Performance ---------
    print("********** Plotting Model Performance Over Time **********")
    plt.figure(figsize=(10, 6))
    for model_name, model_accuracies in accuracies.items():
        plt.plot(model_accuracies, label=model_name)

    plt.title("Model Accuracy Over Multiple Runs")
    plt.xlabel("Run")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.show()

# --------- Run the Monitoring ---------
monitor_performance()

"""**Please Note!**:

Here we have introduced some drift intot the dataset for actual model retraining use case by adding some noise to the data :

We have simulated the drift once after the first model training, and from that point onward, weâ€™ll check for drift in subsequent runs.

For subsequent runs, drift won't be introduced again, only the drift detection will be performed using the already drifted test data.

Install required libraries for mlflow to run
"""

!pip install mlflow pyngrok

!ngrok authtoken 2uOwynufhlwclWuLn9sAfY4mMEf_7LCUJLgrgRsqiTpBD4Z4s

!pip install pyngrok

"""### Run MLflow on port 5000"""

!mlflow ui --port 8002

!gunicorn --bind 0.0.0.0:8001 mlflow:app

"""### Load MLflow and Fetch Experiment Details"""

import os
# Set the path for storing mlruns (you can choose any path)
mlruns_dir = '/content/mlruns'

# Set the MLFLOW_TRACKING_URI to the directory where mlruns will be stored
os.environ['MLFLOW_TRACKING_URI'] = mlruns_dir

# Make sure the directory exists
os.makedirs(mlruns_dir, exist_ok=True)

!pip install mlflow

import mlflow
import mlflow.keras
from mlflow.tracking import MlflowClient
import pandas as pd

# Initialize MLflow client
client = MlflowClient()

# Get Experiment ID (replace with your actual experiment name)
experiment_name = "Fashion_MNIST_Model"
experiment = client.get_experiment_by_name(experiment_name)
experiment_id = experiment.experiment_id

# Fetch all runs from the experiment
runs = client.search_runs(experiment_id, order_by=["metrics.test_accuracy DESC"])

# Convert to Pandas DataFrame for analysis
df = pd.DataFrame([{
    "run_id": run.info.run_id,
    "test_accuracy": run.data.metrics["accuracy"],
    "start_time": run.info.start_time,
    "duration": run.info.end_time - run.info.start_time

} for run in runs])


# Convert start_time to datetime format for better visualization
df["start_time"] = pd.to_datetime(df["start_time"], unit="ms")
pd.set_option('display.width', None)  # This removes the width limit for display

# Apply styling to center both headers and data
df_styled = df.style.set_table_styles([
    {'selector': 'th', 'props': [('text-align', 'center')]},  # Center headers
    {'selector': 'td', 'props': [('text-align', 'center')]},  # Center data cells
])

df_styled

"""### **Visualize Model Accuracy Performance**:


"""

import matplotlib.pyplot as plt

# Sort by test accuracy
df_sorted = df.sort_values("test_accuracy", ascending=False)

# Plot accuracy comparison
plt.figure(figsize=(10, 5))
plt.barh(df_sorted["run_id"], df_sorted["test_accuracy"], color="blue")
plt.xlabel("Test Accuracy")
plt.ylabel("Run ID")
plt.title("Comparison of MLflow Runs by Test Accuracy")
plt.gca().invert_yaxis()  # Best model at the top
plt.show()

"""### Load the Best Model for Deployment

"""

import mlflow.keras

best_run_id = df_sorted.iloc[0]["run_id"]  # Get best run based on accuracy
# Get the details of the best run
best_run = mlflow.get_run(best_run_id)

# Check the available flavors of the model
print(best_run.data.params)
print(best_run.data.metrics)
print(best_run.info.artifact_uri)

# Check the model's saved flavors
# model_conf = best_run.data.artifacts['model']  # 'model' is the name given to the artifact when logged
# model_conf = best_run.info.artifacts_uri['model']  # 'model' is the name given to the artifact when logged

model_path = f"{best_run.info.artifact_uri}/model"  # Adjust path to include model folder
print("Loading model from:", model_path)

# Load the best model using mlflow.keras.load_model
best_model = mlflow.sklearn.load_model(model_path)

best_model

"""### Identify Sudden Drops in Accuracy, drift detection"""

# Define threshold for significant accuracy drop (e.g., 5% or 0.05)
accuracy_drop_threshold = 0.05

# Compute accuracy change between consecutive runs
df["accuracy_change"] = df["test_accuracy"].diff()

# Identify runs where accuracy drops beyond the threshold
drift_runs = df[df["accuracy_change"] < -accuracy_drop_threshold]

if not drift_runs.empty:
    print("âš ï¸ Possible data drift detected in the following runs:")
    print(drift_runs)
else:
    print("âœ… No significant accuracy drops detected.")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(df["start_time"], df["test_accuracy"], marker="o", linestyle="-", label="Test Accuracy")
plt.axhline(y=df["test_accuracy"].max() - accuracy_drop_threshold, color="red", linestyle="--", label="Drift Threshold")
plt.xlabel("Run Timestamp")
plt.ylabel("Test Accuracy")
plt.title("Accuracy Over Time - Drift Detection")
plt.legend()
plt.xticks(rotation=45)
plt.show()

"""## **MLFlow Experiment Tracking Report: Fashion MNIST**
### **1. Experiment Overview**
- **Experiment Name**: `FashionMNIST-Tracking`
- **Total Runs**:   8 for each model 2 times with after drift detected and retrained the model.
- **Tracked Metrics**: `test_accuracy`, `duration`, `start_time`
- **Tracked Model**: `RandomForest,SVM,LogisticRegression and KNeighbors`

---

### **2. Run Performance Summary**
|Run Id	|Test Accuracy	|Start_time |Duration|
|--------|--------------|-----------|--------------|
| b2d9016c3567437587ad359c2354d9bf | 0.855400 |	2025-03-16 16:28:08.095000	| 46946 |
3e35d82a79e846aca9cc3f61f841d1ed | 0.855400 |	2025-03-16 16:27:09.325000 | 46688 |
1baa60c223094926a060da57fc6b0fb3 | **0.882900**	| 2025-03-16 16:16:50.117000 |	619202 |
c243751897544c9b83eb7628fb9d4fd9 |	0.882900 |	2025-03-16 16:06:09.687000 |	628335 |
8a476d872425413182e2c8f561e9808e |	0.844800 | 2025-03-16 16:02:39.585000	 | 210098 |
59af74f4ee2d4c5180b0d9ce9c7f61f8 |	0.844800 |	2025-03-16 15:58:58.355000 |	209116 |
58b7c35aae6549a0bdcf3de969be6dc5 |	0.876400 |	2025-03-16 15:56:51.613000 |	126735 |
6523b7d975a941239235bab6baa24808 |	0.876400 |	2025-03-16 15:54:21.287000 |	138139 |


ðŸ“Œ **Best Accuracy:** `88.00%` (Run ID: `1baa60c223094926a060da57fc6b0fb3`)  
ðŸ“Œ **Worst Accuracy:** `84.48%` (Run ID: `8a476d872425413182e2c8f561e9808e`)  
ðŸ“Œ **No sudden accuracy drop detected** across all 8 runs.  

---


### **4. Model Accuracy Trend Analysis**
âœ… **No sudden accuracy drop detected** across runs.  
âœ… **No significant data drift observed** (threshold: 5% drop).  
âœ… **Top 5 performing models** show accuracy > `85%`.  

---
"""